{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFtzsLuWb0JMQpuGERB8hY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hargurjeet/Adhoc-Activities/blob/main/RAG_with_oops_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOpLGSRgjDwz",
        "outputId": "4ee20bb0-d289-471e-a58f-11fa22734a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community pypdf faiss-cpu sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start_rag_here.py\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "\n",
        "class DocumentIngestor:\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "        )\n",
        "\n",
        "    def load_and_chunk(self, file_path: str) -> List[Document]:\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        raw_docs = loader.load()\n",
        "        return self.splitter.split_documents(raw_docs)\n",
        "\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self):\n",
        "        self.embedding_model = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "    def embed_documents(self, documents: List[Document]) -> FAISS:\n",
        "        return FAISS.from_documents(documents, self.embedding_model)\n",
        "\n",
        "llm_pipeline = pipeline(\"text-generation\", model=\"google/flan-t5-small\")\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "class Retriever:\n",
        "    def __init__(self, vector_store: FAISS):\n",
        "        self.retriever = vector_store.as_retriever()\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n",
        "        return self.retriever.get_relevant_documents(query)[:top_k]\n",
        "\n",
        "\n",
        "class Scorer:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def score_documents(self, query: str, docs: List[Document]) -> List[Document]:\n",
        "        query_emb = self.embedding_model.embed_query(query)\n",
        "        scored = []\n",
        "        for doc in docs:\n",
        "            doc_emb = self.embedding_model.embed_query(doc.page_content)\n",
        "            score = self.cosine_similarity(query_emb, doc_emb)\n",
        "            doc.metadata['score'] = score\n",
        "            scored.append(doc)\n",
        "        return sorted(scored, key=lambda d: d.metadata['score'], reverse=True)\n",
        "\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        from numpy import dot\n",
        "        from numpy.linalg import norm\n",
        "        return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, file_path: str):\n",
        "        self.ingestor = DocumentIngestor()\n",
        "        self.embedder = Embedder()\n",
        "\n",
        "        print(\"Loading and chunking documents...\")\n",
        "        chunks = self.ingestor.load_and_chunk(file_path)\n",
        "\n",
        "        print(\"Embedding documents...\")\n",
        "        self.vector_store = self.embedder.embed_documents(chunks)\n",
        "        self.retriever = Retriever(self.vector_store)\n",
        "        self.scorer = Scorer(self.embedder.embedding_model)\n",
        "\n",
        "    def run(self, query: str, top_k: int = 3):\n",
        "        print(\"Retrieving relevant documents...\")\n",
        "        retrieved_docs = self.retriever.retrieve(query, top_k=top_k * 2)\n",
        "\n",
        "        print(\"Scoring documents...\")\n",
        "        scored_docs = self.scorer.score_documents(query, retrieved_docs)\n",
        "\n",
        "        print(\"Top N Documents:\")\n",
        "        for i, doc in enumerate(scored_docs[:top_k]):\n",
        "            print(f\"\\nRank {i+1} (Score: {doc.metadata['score']:.4f}):\")\n",
        "            print(doc.page_content[:300])  # limit output for brevity\n",
        "\n",
        "\n",
        "# === Example Run ===\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/Hargurjeet _GenAI_Resume.pdf\"  # replace with your document\n",
        "    query = \"tell me about hargurjeet\"\n",
        "    pipeline = RAGPipeline(file_path)\n",
        "    pipeline.run(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-24CMGqcjmak",
        "outputId": "a7be5a5b-5784-4b8e-93c6-861b82bb015e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and chunking documents...\n",
            "Embedding documents...\n",
            "Retrieving relevant documents...\n",
            "Scoring documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-08f56020d2df>:49: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return self.retriever.get_relevant_documents(query)[:top_k]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top N Documents:\n",
            "\n",
            "Rank 1 (Score: 0.3369):\n",
            "Hargurjeet  Singh  Ganger  gurjeet333@gmail.com  |  +91  9035828125  |  Bangalore,  India  linkedin.com/in/hargurjeet/  |  github.com/hargurjeet |  gurjeet333.medium.com  \n",
            "Summary  \n",
            " \n",
            "Experienced  IT  professional  with  14+  years  in  the  industry,  specializing  in  data  science,  statistical  \n",
            "\n",
            "Rank 2 (Score: 0.1609):\n",
            "Warehousing/ETL\n",
            " \n",
            "and\n",
            " \n",
            "relational\n",
            " \n",
            "Databases\n",
            ".\n",
            " \n",
            " ●  Applied  statistics  techniques  such  as  classiﬁcation,  clustering,  statistical  inference ,  and  understanding  of  \n",
            "central\n",
            " \n",
            "tendency.\n",
            " \n",
            "Worked\n",
            " \n",
            "with\n",
            " \n",
            "DL\n",
            " \n",
            "libraries\n",
            " \n",
            "scikit-learn,\n",
            " \n",
            "TensorFlow,\n",
            " \n",
            "Keras,\n",
            " \n",
            "PyTorch...etc.\n",
            " ●  Experienc\n",
            "\n",
            "Rank 3 (Score: 0.1379):\n",
            "EDUCATION   LIVERPOOL  JOHN  MOORES  UNIVERSITY                                                                                       2023  -  2025  M.S.  in  Machine  Learning  &  Artiﬁcial  Intelligence    International  Institute  of  Information  Technology  Bangalore                            \n"
          ]
        }
      ]
    }
  ]
}