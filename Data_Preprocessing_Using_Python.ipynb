{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpOuXZuPI53slDX3lD6vrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hargurjeet/Adhoc-Activities/blob/main/Data_Preprocessing_Using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPROCESSING USING PYTHON**"
      ],
      "metadata": {
        "id": "4JvunhTtdG7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook comprises a collection of commonly used preprocessing techniques that are essential for conducting effective data analysis and data science activities."
      ],
      "metadata": {
        "id": "wF4LDfxedPo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data imports\n",
        "import pandas as pd\n",
        "\n",
        "## Data manuplation\n",
        "import numpy as np\n",
        "\n",
        "## Imputing and preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from scipy import stats\n",
        "\n",
        "## building Visuals\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ouamcShjja2v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Data**"
      ],
      "metadata": {
        "id": "cn1mB4X034c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = pd.read_csv(\"https://raw.githubusercontent.com/hargurjeet/Adhoc-Activities/main/Titanic_train.csv\")\n",
        "titanic_df.head()"
      ],
      "metadata": {
        "id": "CD_9lyrH3-69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling Missing Values and Outliers**"
      ],
      "metadata": {
        "id": "6XjazxzGifwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.info()"
      ],
      "metadata": {
        "id": "qdzruIQz3h0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Observation - \n",
        "- Age and Cabin columns seems to have missing data"
      ],
      "metadata": {
        "id": "2-Rbrj8l4EdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mean Imputation**"
      ],
      "metadata": {
        "id": "E_Rw83cMjkdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values with mean\n",
        "imputer = SimpleImputer(strategy='mean') ## Startegy can be mode and median based on the requirement\n",
        "titanic_df[['Age']] = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Verify the changes\n",
        "print(titanic_df['Age'].isnull().sum())"
      ],
      "metadata": {
        "id": "D7LldmV14ZYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imputation Using ML Algo**"
      ],
      "metadata": {
        "id": "lJWPtaj9jkf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZMaJzw0clKN"
      },
      "outputs": [],
      "source": [
        "titanic_df = pd.read_csv(\"https://raw.githubusercontent.com/hargurjeet/Adhoc-Activities/main/Titanic_train.csv\")\n",
        "titanic_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the IterativeImputer\n",
        "imputer = IterativeImputer(random_state=0)\n",
        "\n",
        "# Fit and transform the data\n",
        "age_imputed = imputer.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Replace the original 'Age' column with the imputed data\n",
        "titanic_df['Age'] = age_imputed\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(titanic_df['Age'].isnull().sum())"
      ],
      "metadata": {
        "id": "AiJsEeH3l72n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Outlier Identification - Visual**"
      ],
      "metadata": {
        "id": "MpzeT406oTP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the plot\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.boxplot(x=titanic_df['Fare'])\n",
        "sns.set(rc={'figure.figsize':(8,6)})\n",
        "plt.xlabel('Fare')\n",
        "plt.title('Boxplot of Fare column in Titanic dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D60AqdxrmSiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Identification - IQR"
      ],
      "metadata": {
        "id": "Nj9FeJaupeTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the IQR\n",
        "q1 = titanic_df['Fare'].quantile(0.25)\n",
        "q3 = titanic_df['Fare'].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the upper and lower bounds\n",
        "lower_bound = q1 - 1.5*iqr\n",
        "upper_bound = q3 + 1.5*iqr\n",
        "\n",
        "# print(lower_bound, upper_bound)\n",
        "# Identify the outliers and filtering them out\n",
        "titanic_df = titanic_df[(titanic_df['Fare'] >= lower_bound) & (titanic_df['Fare'] <= upper_bound)]\n",
        "\n",
        "# Show the plot\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.boxplot(x=titanic_df['Fare'])\n",
        "sns.set(rc={'figure.figsize':(8,6)})\n",
        "plt.xlabel('Fare')\n",
        "plt.title('Boxplot of Fare column in Titanic dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MwXLJ8RAmSfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Identification - Z Score"
      ],
      "metadata": {
        "id": "5yhSZfc0j3k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = pd.read_csv(\"https://raw.githubusercontent.com/hargurjeet/Adhoc-Activities/main/Titanic_train.csv\")\n",
        "\n",
        "z_scores = stats.zscore(titanic_df['Fare'])\n",
        "abs_z_scores = np.abs(z_scores)\n",
        "\n",
        "# Setting the threshold z-score value to 3\n",
        "threshold = 3\n",
        "outlier_indices = np.where(abs_z_scores > threshold)[0]\n",
        "outlier_values = titanic_df['Fare'][outlier_indices]\n",
        "\n",
        "print(\"Number of outliers:\", len(outlier_values))\n",
        "print(\"Outlier values:\", outlier_values)"
      ],
      "metadata": {
        "id": "1TipYjCK9e1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling Categorical Data**"
      ],
      "metadata": {
        "id": "7z2qicEP_Y-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mode Imputation"
      ],
      "metadata": {
        "id": "e6WcuL2GkI97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Impute missing values in 'Sex' and 'Embarked' columns with most frequent value\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "titanic_df[['Embarked']] = imputer.fit_transform(titanic_df[['Embarked']])\n",
        "\n",
        "# Verify the changes\n",
        "print(titanic_df[['Embarked']].isnull().sum())"
      ],
      "metadata": {
        "id": "2jhbqq1S_gEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Imputation, Missing Category and Label Encoding"
      ],
      "metadata": {
        "id": "8yEDfJRV-8ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = pd.read_csv(\"https://raw.githubusercontent.com/hargurjeet/Adhoc-Activities/main/Titanic_train.csv\")\n",
        "\n",
        "# print the null records\n",
        "print(titanic_df[['Embarked']].isnull().sum())\n",
        "\n",
        "# Encode the 'Sex' column\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode the 'Embarked' column\n",
        "titanic_df['Embarked'] = titanic_df['Embarked'].fillna('Unknown')\n",
        "titanic_df['Embarked'] = le.fit_transform(titanic_df['Embarked'])\n",
        "\n",
        "# define the KNN imputer with k=5\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# impute missing values in the 'Sex' and 'Embarked' columns\n",
        "titanic_imputed = imputer.fit_transform(titanic_df[['Embarked']])\n",
        "\n",
        "# print the imputed dataset\n",
        "titanic_df['Embarked'] = titanic_imputed\n",
        "print(titanic_df[['Embarked']].isnull().sum())"
      ],
      "metadata": {
        "id": "4EjkakTaE1SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categorical Data - Encoding Techniques**"
      ],
      "metadata": {
        "id": "CaW0e9JHkUli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoding"
      ],
      "metadata": {
        "id": "cCDKGQspkeTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the OneHotEncoder class\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "\n",
        "# One hot encode 'Sex' and 'Embarked' columns\n",
        "encoded_cols = encoder.fit_transform(titanic_df[['Sex', 'Embarked']])\n",
        "\n",
        "# Convert the encoded columns to a dataframe and append to the original dataframe\n",
        "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(['Sex', 'Embarked']))\n",
        "titanic_df = pd.concat([titanic_df, encoded_df], axis=1)"
      ],
      "metadata": {
        "id": "1JStXv6F_f-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df"
      ],
      "metadata": {
        "id": "GdxTztz2_f7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Scaling**"
      ],
      "metadata": {
        "id": "Ey_R3lLl_NId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Min Max scaler"
      ],
      "metadata": {
        "id": "sDe8GBNC_R5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = pd.read_csv(\"https://raw.githubusercontent.com/hargurjeet/Adhoc-Activities/main/Titanic_train.csv\")\n",
        "titanic_df.describe()"
      ],
      "metadata": {
        "id": "uh08CxNHJTcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select numerical columns\n",
        "num_cols = ['Age', 'Fare']\n",
        "\n",
        "# create StandardScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit and transform the data\n",
        "titanic_df[num_cols] = scaler.fit_transform(titanic_df[num_cols])\n",
        "\n",
        "# print the standardized data\n",
        "print(titanic_df[num_cols].describe())"
      ],
      "metadata": {
        "id": "aPgTng07Jyw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log tranformations"
      ],
      "metadata": {
        "id": "vJHai9Vn_R1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = pd.read_csv(\"https://raw.githubusercontent.com/hargurjeet/Adhoc-Activities/main/Titanic_train.csv\")\n",
        "\n",
        "# Apply log transformation to the \"Fare\" column\n",
        "titanic_df_log = np.log(titanic_df['Fare']).copy()\n",
        "\n",
        "sns.histplot(data= titanic_df, x= 'Fare')"
      ],
      "metadata": {
        "id": "DTZHOVqyLdMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot( x=titanic_df_log)"
      ],
      "metadata": {
        "id": "5-aYB28rUUOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Power tranformations"
      ],
      "metadata": {
        "id": "S66XRrFyVNaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Generate some skewed data\n",
        "data = np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "# Visualize the data distribution\n",
        "plt.hist(data, bins=50)\n",
        "plt.title(\"Original data distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Apply power transformation\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "data_pt = pt.fit_transform(data.reshape(-1, 1))\n",
        "\n",
        "# Visualize the transformed data distribution\n",
        "plt.hist(data_pt, bins=50)\n",
        "plt.title(\"Power-transformed data distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vEzesmRtVSiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCVwwO_fVSfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reference Material**\n",
        "\n",
        "- Read menthod pandas - https://realpython.com/pandas-read-write-files/\n",
        "- Sklearn Imputer - https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "- Sklearn pre processing - https://scikit-learn.org/stable/modules/preprocessing.html"
      ],
      "metadata": {
        "id": "geG2fPBAkWOQ"
      }
    }
  ]
}