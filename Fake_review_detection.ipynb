{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CYX5VaMjCjz6_DCF4iQkFpe5eBKM2xmB",
      "authorship_tag": "ABX9TyMv31WQQ6o9gGJMhwTOoV0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hargurjeet/Adhoc-Activities/blob/main/Fake_review_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDNLkk1Tzvyo",
        "outputId": "5c8ae70d-40ec-444c-9ecc-cbc7271106be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as py\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def unzip_file(zip_file_path, output_dir):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "# Example usage:\n",
        "zip_file_path = '/content/ReviewerNamewise.zip'  # Replace this with the path to your zip file\n",
        "output_dir = '/content/'  # Replace this with the directory where you want to extract the files\n",
        "unzip_file(zip_file_path, output_dir)"
      ],
      "metadata": {
        "id": "nYkbfWwp0hvi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def read_and_append_csv_files(folder_path):\n",
        "    all_data = pd.DataFrame()  # Initialize an empty DataFrame to store the combined data\n",
        "\n",
        "    # List all files in the folder\n",
        "    file_list = os.listdir(folder_path)\n",
        "\n",
        "    # Loop through each file\n",
        "    for file_name in file_list:\n",
        "        if file_name.endswith('.csv'):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(folder_path, file_name)  # Get the full file path\n",
        "            data = pd.read_csv(file_path)  # Read the CSV file\n",
        "            all_data = all_data.append(data, ignore_index=True)  # Append the data to the DataFrame\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Example usage:\n",
        "folder_path = '/content/ReviewerNamewise/NormalCase'  # Replace this with the path to your folder containing CSV files\n",
        "combined_data = read_and_append_csv_files(folder_path)\n"
      ],
      "metadata": {
        "id": "zhvPRJ0l09Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMD4mf9_1hss",
        "outputId": "28d7024e-f617-4fbf-cad7-0c8b21247d9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(644, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = load('/content/drive/MyDrive/Documents/models.joblib')"
      ],
      "metadata": {
        "id": "Ey94yahk1mua"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvK0bUsB37S-",
        "outputId": "e5aafef7-ba34-467b-f256-67ccf7a5fbb9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "               colsample_bylevel=None, colsample_bynode=None,\n",
              "               colsample_bytree=None, early_stopping_rounds=None,\n",
              "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "               interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
              "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "               predictor=None, random_state=None, ...),\n",
              " RandomForestClassifier()]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# Define a function to preprocess the review text\n",
        "def preprocess_text(text):\n",
        "    # Convert the text to lowercase and split it into words\n",
        "    words = text.lower().split()\n",
        "\n",
        "    # Remove punctuation and numbers from the words\n",
        "    words = [word.translate(str.maketrans('', '', string.punctuation + string.digits)) for word in words]\n",
        "\n",
        "    # Remove stop words from the words\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "\n",
        "    # Stem the words using a Porter stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Return the preprocessed text as a list of words\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "gnpQaxGt2aqy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def get_word2vec_embedding(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Train a word2vec model on the tokenized words\n",
        "    model = Word2Vec([words], min_count=1, vector_size=100)\n",
        "\n",
        "    # Get the word2vec embeddings for the words\n",
        "    word_vectors = model.wv\n",
        "\n",
        "    # Return the word2vec embeddings\n",
        "    return word_vectors"
      ],
      "metadata": {
        "id": "tM7AET9A2eoy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_comment=\"This is an example sentence.\"\n",
        "pre_new_comment = preprocess_text(new_comment)\n",
        "embeddings = get_word2vec_embedding(pre_new_comment)\n",
        "\n",
        "# Data type of array element\n",
        "Data_type = float\n",
        "\n",
        "# This cause Value error\n",
        "np_array = np.array(embeddings, dtype=Data_type)\n",
        "\n",
        "\n",
        "# embeddings = np.reshape(embeddings, (1, -1))\n",
        "# Predict the class label for the new record\n",
        "# predicted_class = classifiers[-1].predict(embeddings)\n",
        "predicted_class = loaded_model[-1].predict(np_array)\n",
        "# Print the predicted class\n",
        "print(predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "3lJaf_tp2Ul8",
        "outputId": "e7d53863-9f87-450e-f918-63b136f1c98a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'KeyedVectors'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e906763c27b6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# This cause Value error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnp_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mData_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWdw-Tk52Ujf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}